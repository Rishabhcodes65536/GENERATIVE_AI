{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Diffusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll see how we can steer our model outputs towards specific types or classes of images. We can use a method called conditioning, where the idea is to ask the model to generate not just any image but an image belonging to a pre-defined class.\n",
    "\n",
    "First, rather than using the butterflies dataset, we’ll switch to a dataset that has classes. We’ll use Fashion MNIST, a dataset with thousands of images of clothes associated with a label from 10 different classes. Then, crucially, we’ll run two inputs through the model. Instead of just showing it what real images look like, we’ll also tell it the class every image belongs to. We expect the model to learn to associate images and labels to understand the distinctive features of sweaters, boots, and the like.\n",
    "\n",
    "Note that we are not interested in solving a classification problem – we don’t want the model to tell us which class the image belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a compact size, black-and-white images, and ten classes. The main difference is that classes correspond to different types of clothing instead of being digits, and the images contain more detail than simple handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from genaibook.core import show_images\n",
    "\n",
    "fashion_mnist = load_dataset(\"fashion_mnist\")\n",
    "clothes = fashion_mnist[\"train\"][\"image\"][:8]\n",
    "classes = fashion_mnist[\"train\"][\"label\"][:8]\n",
    "show_images(clothes, titles=classes, figsize=(4, 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of resizing, we’ll pad our image inputs (28 × 28 pixels) to 32 × 32. This will preserve the original image quality, which will help the UNet make higher quality predictions.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),  # Randomly flip (data augmentation)\n",
    "        transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
    "        transforms.Pad(2),  # Add 2 pixels on all sides\n",
    "        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(image) for image in examples[\"image\"]]\n",
    "    return {\"images\": images, \"labels\": examples[\"label\"]}\n",
    "\n",
    "\n",
    "train_dataset = fashion_mnist[\"train\"].with_transform(transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=256, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creatin a class conditioned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UNet from the diffusers library allows providing custom conditioning information.we add a num_class_embeds argument to the UNet constructor. This argument tells the model we’d like to use class labels as additional conditioning. We’ll use ten as that’s the number of classes in Fashion MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    in_channels=1,  # 1 channel for grayscale images\n",
    "    out_channels=1,\n",
    "    sample_size=32,\n",
    "    block_out_channels=(32, 64, 128, 256),\n",
    "    num_class_embeds=10,  # Enable class conditioning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions with this model, we must pass in the class labels as additional inputs to the forward() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 1, 32, 32))\n",
    "with torch.no_grad():\n",
    "    out = model(x, timestep=7, class_labels=torch.tensor([2])).sample\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also pass something else to the model as conditioning: the timestep! That’s right, even the model from Diffusion chapter can be considered a conditional diffusion model. We condition it on the timestep, expecting that knowing how far we are in the diffusion process will help it generate more realistic images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, the timestep and the class label are turned into embeddings that the model uses during its forward pass. At multiple stages throughout the UNet, these embeddings are projected onto a dimension that matches the number of channels in a given layer. The embeddings are then added to the outputs of that layer. This means the conditioning information is fed to every block of the UNet,\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding noise works just as well on greyscale images as on the butterflies from Chapter 4. Let’s look at the impact of noise as we do more noising timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "\n",
    "scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02\n",
    ")\n",
    "timesteps = torch.linspace(0, 999, 8).long()\n",
    "batch = next(iter(train_dataloader))\n",
    "x = batch[\"images\"][0].expand([8, 1, 32, 32])\n",
    "noise = torch.rand_like(x)\n",
    "noised_x = scheduler.add_noise(x, noise, timesteps)\n",
    "show_images((noised_x * 0.5 + 0.5).clip(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training loop is also almost the same as in Chapter 4, except that we now pass the class labels for conditioning. Note that this is just additional information for the model, but it doesn’t affect how we define our loss function in any way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll also display some progress during training using the Python package tqdm. tqdm means “progress” in Arabic (taqadum, تقدّم) and is an abbreviation for “I love you so much” in Spanish (te quiero demasiado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Loads a batch of images and their corresponding labels.\n",
    "\n",
    "2.Adds noise to the images based on their timestep.\n",
    "\n",
    "3.Feeds the noisy images into the model, alongside the class labels for conditioning.\n",
    "\n",
    "4.Calculates the loss.\n",
    "5.Backpropagates the loss and updates the model weights with the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02\n",
    ")\n",
    "\n",
    "num_epochs = 25\n",
    "lr = 3e-4\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, eps=1e-5)\n",
    "losses = []  # Somewhere to store the loss values for later plotting\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Train the model (this takes a while!)\n",
    "for epoch in (progress := tqdm(range(num_epochs))):\n",
    "    for step, batch in (\n",
    "        inner := tqdm(\n",
    "            enumerate(train_dataloader),\n",
    "            position=0,\n",
    "            leave=True,\n",
    "            total=len(train_dataloader),\n",
    "        )\n",
    "    ):\n",
    "        # Load the input images and classes\n",
    "        clean_images = batch[\"images\"].to(device)\n",
    "        class_labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Sample noise to add to the images\n",
    "        noise = torch.randn(clean_images.shape).to(device)\n",
    "\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0,\n",
    "            scheduler.config.num_train_timesteps,\n",
    "            (clean_images.shape[0],),\n",
    "            device=device,\n",
    "        ).long()\n",
    "\n",
    "        # Add noise to the clean images according to the timestep\n",
    "        noisy_images = scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        # Get the model prediction for the noise - note the use of class_labels\n",
    "        noise_pred = model(\n",
    "            noisy_images,\n",
    "            timesteps,\n",
    "            class_labels=class_labels,\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "\n",
    "        # Compare the prediction with the actual noise:\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Display loss\n",
    "        inner.set_postfix(loss=f\"{loss.cpu().item():.3f}\")\n",
    "\n",
    "        # Store the loss for later plotting\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Update the model parameters with the optimizer based on this loss\n",
    "        loss.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We now have a model that expects two inputs when making predictions: the image and the class label. We can create samples by beginning with random noise and then iteratively denoising, passing in whatever class label we’d like to generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_class(class_to_generate, n_samples=8):\n",
    "    sample = torch.randn(n_samples, 1, 32, 32).to(device)\n",
    "    class_labels = [class_to_generate] * n_samples\n",
    "    class_labels = torch.tensor(class_labels).to(device)\n",
    "\n",
    "    for _, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "        # Get model pred\n",
    "        with torch.no_grad():\n",
    "            noise_pred = model(sample, t, class_labels=class_labels).sample\n",
    "\n",
    "        # Update sample with step\n",
    "        sample = scheduler.step(noise_pred, t, sample).prev_sample\n",
    "\n",
    "    return sample.clip(-1, 1) * 0.5 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate t-shirts (class 0)\n",
    "images = generate_from_class(0)\n",
    "show_images(images, nrows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now generate some sneakers (class 7)\n",
    "images = generate_from_class(7)\n",
    "show_images(images, nrows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...or boots (class 9)\n",
    "images = generate_from_class(9)\n",
    "show_images(images, nrows=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Improving efficiency with Latent Diffusion Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As image size grows, so does the computational power required to work with those images. This is especially pronounced in self-attention, where the amount of operations grows quadratically with the number of inputs. A 128px square image has four times as many pixels as a 64px square image, requiring 16 times the memory and computing in a self-attention layer. This is a problem for anyone who’d like to generate high-resolution images.\n",
    "\n",
    "Latent diffusion tries to mitigate this issue using a separate Variational Auto-Encoder. As we saw in Chapter 2, VAEs can compress images to a smaller spatial dimension.\n",
    "\n",
    "The VAE used in Stable Diffusion takes in 3-channel images and produces a 4-channel latent representation with a reduction factor of 8 for each spatial dimension. A 512px input square image (3x512x512=786,432 values) will be compressed down to a 4x64x64 latent (16,384 values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, StableDiffusionPipeline\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16\n",
    ").to(device)\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"Watercolor illustration of a rose\").images[0]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
