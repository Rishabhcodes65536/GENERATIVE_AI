{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizing text\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "input_ids = tokenizer(\"It was a dark and stormy\", return_tensors=\"pt\").input_ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in input_ids[0]:\n",
    "    print(t, \"\\t:\", tokenizer.decode(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting Probabilities -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = gpt2(input_ids)\n",
    "outputs.logits.shape  # An output for each input token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_logits = gpt2(input_ids).logits[0, -1]  # The last set of logits\n",
    "final_logits.argmax()  # The position of the maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(final_logits.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "top10_logits = torch.topk(final_logits, 10)\n",
    "for index in top10_logits.indices:\n",
    "    print(tokenizer.decode(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = torch.topk(final_logits.softmax(dim=0), 10)\n",
    "for value, index in zip(top10.values, top10.indices):\n",
    "    print(f\"{tokenizer.decode(index):<10} {value.item():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Text -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = gpt2.generate(input_ids, max_new_tokens=20)\n",
    "decoded_text = tokenizer.decode(output_ids[0])\n",
    "\n",
    "print(\"Input IDs\", input_ids[0])\n",
    "print(\"Output IDs\", output_ids)\n",
    "print(f\"Generated text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_output = gpt2.generate(\n",
    "    input_ids,\n",
    "    num_beams=5,\n",
    "    max_new_tokens=30,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_output = gpt2.generate(\n",
    "    input_ids,\n",
    "    num_beams=5,\n",
    "    repetition_penalty=1.2,\n",
    "    max_new_tokens=38,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "# Setting the seed ensures we get the same results every time we run this code\n",
    "set_seed(70)\n",
    "\n",
    "sampling_output = gpt2.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=34,\n",
    "    top_k=0,  # We'll come back to this parameter\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_output = gpt2.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.4,\n",
    "    max_length=40,\n",
    "    top_k=0,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_output = gpt2.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.001,\n",
    "    max_length=40,\n",
    "    top_k=0,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_output = gpt2.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=3.0,\n",
    "    max_length=40,\n",
    "    top_k=0,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_output = gpt2.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=40,\n",
    "    top_k=10,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_output = gpt2.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=40,\n",
    "    top_p=0.94,\n",
    "    top_k=0,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero Shot Generalisation -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the token IDs for the words ' positive' and ' negative'\n",
    "# (note the space before the words)\n",
    "tokenizer.encode(\" positive\"), tokenizer.encode(\" negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(review):\n",
    "    \"\"\"Predict whether it is positive or negative\n",
    "\n",
    "    This function predicts whether a review is positive or negative\n",
    "    using a bit of clever prompting. It looks at the logits for the\n",
    "    tokens ' positive' and ' negative' (note the space before the\n",
    "    words), and returns the label with the highest score.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Question: Is the following review positive or\n",
    "negative about the movie?\n",
    "Review: {review} Answer:\"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    final_logits = gpt2(input_ids).logits[0, -1]\n",
    "    if final_logits[3967] > final_logits[4633]:\n",
    "        print(\"Positive\")\n",
    "    else:\n",
    "        print(\"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(\"This movie was terrible!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(\"That was a delight to watch, 10/10 would recommend :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few Shot Generalisation -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\\n",
    "Translate English to Spanish:\n",
    "\n",
    "English: I do not speak Spanish.\n",
    "Spanish: No hablo español.\n",
    "\n",
    "English: See you later!\n",
    "Spanish: ¡Hasta luego!\n",
    "\n",
    "English: Where is a good restaurant?\n",
    "Spanish: ¿Dónde hay un buen restaurante?\n",
    "\n",
    "English: What rooms do you have available?\n",
    "Spanish: ¿Qué habitaciones tiene disponibles?\n",
    "\n",
    "English: I like soccer\n",
    "Spanish:\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "output = model.generate(\n",
    "    inputs,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=10,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
