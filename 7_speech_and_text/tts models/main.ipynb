{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5ForSpeechToText, SpeechT5Processor\n",
    "\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_asr\")\n",
    "model = SpeechT5ForSpeechToText.from_pretrained(\"microsoft/speecht5_asr\")\n",
    "\n",
    "inputs = processor(\n",
    "    audio=array, sampling_rate=sampling_rate, return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(**inputs, max_new_tokens=200)\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['chapter sixteen i might have told you of the beginning i might have told you of the beginning of the beginning of the beginning of the beginning of the beginning chapter sixteen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5ForTextToSpeech\n",
    "\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "\n",
    "inputs = processor(text=\"There are llamas all around.\", return_tensors=\"pt\")\n",
    "embeddings_dataset = load_dataset(\n",
    "    \"Matthijs/cmu-arctic-xvectors\", split=\"validation\"\n",
    ")\n",
    "speaker_embeddings = torch.tensor(\n",
    "    embeddings_dataset[7440][\"xvector\"]\n",
    ").unsqueeze(0)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    spectrogram = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings)\n",
    "spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor([[-3.6003, -3.6643, -3.7114,  ..., -4.5534, -4.5948, -4.7614],\n",
    "        [-3.3392, -3.4059, -3.4096,  ..., -4.4603, -4.4640, -4.7025],\n",
    "        [-2.9305, -3.0289, -3.0103,  ..., -4.1975, -4.2770, -4.5122],\n",
    "        ...,\n",
    "        [-3.2018, -3.4044, -3.5426,  ..., -4.5240, -4.5532, -4.7528],\n",
    "        [-3.2666, -3.4596, -3.5791,  ..., -4.5361, -4.5633, -4.7589],\n",
    "        [-3.2899, -3.4900, -3.6032,  ..., -4.5457, -4.5672, -4.7564]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert spectrogram tensor to numpy array\n",
    "spectrogram_np = spectrogram.cpu().numpy()\n",
    "\n",
    "# Display the spectrogram using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(spectrogram_np[0], aspect='auto', origin='lower', cmap='inferno')\n",
    "plt.colorbar(label='Intensity')\n",
    "plt.title('Spectrogram')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](spectogram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5HifiGan\n",
    "\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "with torch.no_grad():\n",
    "    # Alternatively\n",
    "    # model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "    speech = vocoder(spectrogram)\n",
    "speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor([-4.7037e-05,  2.6946e-05,  1.9352e-05,  ..., -1.9595e-04,\n",
    "        -1.6578e-04, -2.3303e-04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "# Convert the tensor to a NumPy array\n",
    "speech_numpy = speech.squeeze().cpu().numpy()\n",
    "\n",
    "# Save the NumPy array as a WAV file\n",
    "sf.write(\"output.wav\", speech_numpy, 16000)\n",
    "\n",
    "# Optionally, play the audio (requires IPython)\n",
    "from IPython.display import Audio\n",
    "Audio(\"output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VitsModel, VitsTokenizer, set_seed\n",
    "\n",
    "tokenizer = VitsTokenizer.from_pretrained(\"facebook/mms-tts-eng\")\n",
    "model = VitsModel.from_pretrained(\"facebook/mms-tts-eng\")\n",
    "\n",
    "inputs = tokenizer(text=\"Hello - my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "set_seed(555)  # make deterministic\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs[\"input_ids\"])\n",
    "\n",
    "outputs.waveform[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the waveform tensor and move it to CPU\n",
    "waveform_tensor = outputs.waveform[0].cpu()\n",
    "\n",
    "# Convert the tensor to a NumPy array\n",
    "waveform_numpy = waveform_tensor.numpy()\n",
    "\n",
    "# Save the NumPy array as a WAV file\n",
    "sf.write(\"mms_output.wav\", waveform_numpy, 16000)  # Assuming the sample rate is 22050\n",
    "\n",
    "# Optionally, play the audio (requires IPython)\n",
    "from IPython.display import Audio\n",
    "Audio(\"mms_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"suno/bark-small\")\n",
    "model = AutoModel.from_pretrained(\"suno/bark-small\").to(device)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\n",
    "        \"Hello, my name is Suno. And, uh â€” and I like pizza. [laughs] But I also have other interests such as playing tic tac toe.\"\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "\n",
    "speech_values = model.generate(**inputs, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the speech values tensor to a NumPy array\n",
    "speech_numpy = speech_values.squeeze().cpu().numpy()\n",
    "\n",
    "# Save the NumPy array as a WAV file\n",
    "sf.write(\"suno_output.wav\", speech_numpy, 16000)  # Adjust the sample rate if necessary\n",
    "\n",
    "# Optionally, play the audio (requires IPython)\n",
    "from IPython.display import Audio\n",
    "Audio(\"suno_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_preset = \"v2/en_speaker_6\"\n",
    "\n",
    "inputs = processor(\"Hello, my dog is cute\", voice_preset=voice_preset).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "audio_array = model.generate(**inputs)\n",
    "audio_array = audio_array.cpu().numpy().squeeze()\n",
    "\n",
    "# Save the NumPy array as a WAV file\n",
    "sf.write(\"6_speaker_output.wav\", audio_array, 16000)  # Adjust the sample rate if necessary\n",
    "\n",
    "# Optionally, play the audio (requires IPython)\n",
    "from IPython.display import Audio\n",
    "Audio(\"6_speaker_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\n",
    "    \"facebook/musicgen-small\"\n",
    ").to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
    "inputs = processor(\n",
    "    text=[\"an intense rock guitar solo\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "audio_values = model.generate(\n",
    "    **inputs, do_sample=False, guidance_scale=3, max_new_tokens=256\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the audio values tensor to a NumPy array\n",
    "audio_numpy = audio_values.squeeze().cpu().numpy()\n",
    "\n",
    "# Save the NumPy array as a WAV file\n",
    "sf.write(\"musicgen_output.wav\", audio_numpy, 8000)  # Adjust the sample rate if necessary\n",
    "\n",
    "# Optionally, play the audio (requires IPython)\n",
    "from IPython.display import Audio\n",
    "Audio(\"musicgen_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "int_device = 0 if torch.cuda.is_available() else -1  # 0 for GPU, -1 for CPU\n",
    "\n",
    "# Use pipeline with adjusted parameters\n",
    "try:\n",
    "    pipe = pipeline(\"text-to-audio\", model=\"facebook/musicgen-small\", device=int_device)\n",
    "\n",
    "    # Adjust generation parameters\n",
    "    data = pipe(\n",
    "        \"electric rock solo, very intense\",\n",
    "    )\n",
    "\n",
    "    # Output the generated audio data\n",
    "    print(data)\n",
    "except Exception as e:\n",
    "    print(f\"Error during pipeline execution: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model and processor manually to check for any issues\n",
    "try:\n",
    "    model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\").to(device)\n",
    "    processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or processor: {e}\")\n",
    "\n",
    "# Prepare inputs\n",
    "try:\n",
    "    inputs = processor(\n",
    "        text=[\"electric rock solo, very intense\"],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing inputs: {e}\")\n",
    "\n",
    "# Function to validate tensors\n",
    "def validate_tensors(inputs):\n",
    "    for key, tensor in inputs.items():\n",
    "        if torch.isnan(tensor).any() or torch.isinf(tensor).any() or (tensor < 0).any():\n",
    "            raise ValueError(f\"Invalid values found in tensor: {key}\")\n",
    "\n",
    "# Generate audio\n",
    "try:\n",
    "    validate_tensors(inputs)\n",
    "    audio_values = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        guidance_scale=3,\n",
    "        max_new_tokens=256\n",
    "    )\n",
    "    print(audio_values)\n",
    "except Exception as e:\n",
    "    print(f\"Error during audio generation: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
