{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder\n",
    "\n",
    "# librispeech_asr dataset contains custom code which must be executed to correctly load the dataset.\n",
    "# You can inspect the repository content at https://hf.co/datasets/librispeech_asr\n",
    "# We add the `trust_remote_code=True` argument to indicate that we trust this code.\n",
    "ds_builder = load_dataset_builder(\"librispeech_asr\", trust_remote_code=True)\n",
    "ds_builder.info.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_builder.info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'file': Value(dtype='string', id=None),\n",
    " 'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None),\n",
    " 'text': Value(dtype='string', id=None),\n",
    " 'speaker_id': Value(dtype='int64', id=None),\n",
    " 'chapter_id': Value(dtype='int64', id=None),\n",
    " 'id': Value(dtype='string', id=None)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"librispeech_asr\",\n",
    "    split=\"train.clean.100\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "sample = next(iter(ds))\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'file': '374-180298-0000.flac',\n",
    " 'audio': {'path': '374-180298-0000.flac',\n",
    "  'array': array([ 7.01904297e-04,  7.32421875e-04,  7.32421875e-04, ...,\n",
    "         -2.74658203e-04, -1.83105469e-04, -3.05175781e-05]),\n",
    "  'sampling_rate': 16000},\n",
    " 'text': 'CHAPTER SIXTEEN I MIGHT HAVE TOLD YOU OF THE BEGINNING OF THIS LIAISON IN A FEW LINES BUT I WANTED YOU TO SEE EVERY STEP BY WHICH WE CAME I TO AGREE TO WHATEVER MARGUERITE WISHED',\n",
    " 'speaker_id': 374,\n",
    " 'chapter_id': 180298,\n",
    " 'id': '374-180298-0000'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = sample[\"audio\"][\"array\"]\n",
    "sampling_rate = sample[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "# Let's get the first 5 seconds\n",
    "array = array[: sampling_rate * 5]\n",
    "print(f\"Number of samples: {len(array)}. Values: {array}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of samples: 80000. Values: [ 0.0007019   0.00073242  0.00073242 ... -0.02697754 -0.02227783\n",
    " -0.0300293 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "\n",
    "librosa.display.waveshow(array, sr=sampling_rate);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](waveform.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def plot_sine(freq):\n",
    "    sr = 1000  # samples per second\n",
    "    ts = 1.0 / sr  # sampling interval\n",
    "    t = np.arange(0, 1, ts)  # time vector\n",
    "    amplitude = np.sin(2 * np.pi * freq * t)\n",
    "\n",
    "    plt.plot(t, amplitude)\n",
    "    plt.title(\"Sine wave wih frequency {}\".format(freq))\n",
    "    plt.xlabel(\"Time\")\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plot_sine(1)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plot_sine(2)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plot_sine(5)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plot_sine(30)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](sine.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.fft.fft(array)\n",
    "N = len(X)\n",
    "n = np.arange(N)\n",
    "T = N / sampling_rate\n",
    "freq = n / T\n",
    "plt.stem(freq[:8000], np.abs(X[:8000]), \"b\", markerfmt=\" \", basefmt=\"-b\")\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Amplitude in Frequency Domain\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](audio_fft.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.abs(librosa.stft(array))\n",
    "S_db = librosa.amplitude_to_db(D, ref=np.max)\n",
    "\n",
    "librosa.display.specshow(S_db, sr=sampling_rate, x_axis=\"time\", y_axis=\"hz\")\n",
    "plt.colorbar(format=\"%+2.0f dB\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](spectogram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(y=array, sr=sampling_rate)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "librosa.display.specshow(S_dB, sr=sampling_rate, x_axis=\"time\", y_axis=\"mel\")\n",
    "plt.colorbar(format=\"%+2.0f dB\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](mel_spectogram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Diffusion based Audio Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AudioDiffusionPipeline\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = AudioDiffusionPipeline.from_pretrained(\n",
    "    \"teticio/audio-diffusion-ddim-256\"\n",
    ").to(device)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "output = pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"riffusion/riffusion-model-v1\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "prompt = \"slow piano piece, classical\"\n",
    "negative_prompt = \"drums\"\n",
    "spec_img = pipe(\n",
    "    prompt, negative_prompt=negative_prompt, height=512, width=512\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"riffusion/riffusion-model-v1\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "prompt = \"guitar, acoustic, calmed\"\n",
    "generator = torch.Generator(device=device).manual_seed(1024)\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    image=spec_img,\n",
    "    strength=0.7,\n",
    "    guidance_scale=8,\n",
    "    generator=generator,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dance Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DanceDiffusionPipeline\n",
    "\n",
    "pipe = DanceDiffusionPipeline.from_pretrained(\n",
    "    \"harmonai/maestro-150k\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "audio = pipe(audio_length_in_s=5, num_inference_steps=50).audios[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech to Text With Transformer-based Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-tiny\",\n",
    "    max_new_tokens=100,\n",
    ")\n",
    "pipe(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
    "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
    "{'text': ' Chapter 16 I might have told you of the beginning'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder only based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In the first part of this chapter we used spectrograms to capture the amplitude and frequency characteristics of the input data in a concise 2D visual representation. In this case, we are using CNNs instead of spectrograms to better manage the vast amounts of input data we need to process. Both are dimensionality reduction techniques, and the choice depends on factors like the task to solve or the architecture we choose. Transformers, thanks to the attention mechanism, are great to handle data sequences, so staying close to a sequential temporal representation seems to make sense.\n",
    "\n",
    "Let’s recap the whole flow to perform ASR with encoder-based models:\n",
    "\n",
    "Raw audio data (1D array) representing the amplitudes is received.\n",
    "\n",
    "Data is normalized to zero mean and univariance to standardize across different amplitudes.\n",
    "\n",
    "A small convolutional neural network turns the audio into a latent representation. This reduces the length of the input sequence.\n",
    "\n",
    "The representations are then passed to an encoder model, which outputs embeddings for each representation.\n",
    "\n",
    "Each embedding is finally passed through a classifier, which predicts the corresponding character for each one.\n",
    "\n",
    "The output of such a model would be something as follows:\n",
    "\n",
    "CHAAAAAPTTERRRSSIXTEEEEENIMMMIIGHT..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "# The AutoProcessor has the pre and post-processing incorporated\n",
    "wav2vec2_processor = Wav2Vec2Processor.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\"\n",
    ")\n",
    "wav2vec2_model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\"\n",
    ").to(device)\n",
    "\n",
    "# Run forward pass, making sure to resample to 16kHz\n",
    "inputs = wav2vec2_processor(\n",
    "    array, sampling_rate=sampling_rate, return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    outputs = wav2vec2_model(**inputs.to(device))\n",
    "\n",
    "# Transcribe\n",
    "predicted_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "transcription = wav2vec2_processor.batch_decode(predicted_ids)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "['CHAPTER SIXTEEN I MIGHT HAVE TOLD YOU OF THE BEGI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper output has a very specific format, so looking at the figure above is essential to understand its generation. It doesn’t just output the transcription but also different special characters that provide significant flexibility for Whisper to perform multiple tasks in multiple languages. This format is not just specific to the output texts but also to the data used during training. Some of the most important tokens are:\n",
    "\n",
    "The speech begins with a start of transcript token.\n",
    "\n",
    "If the language is not English, there is a language tag token (e.g., hi for Hindi).\n",
    "\n",
    "With the language tag, one can perform language identification, transcription, or translate to English.\n",
    "\n",
    "If there’s a no speech token, Whisper is used for voice activity detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can present the ASR problem as a sequence-to-sequence problem rather than a classification problem. This is what Whisper, the open-source model we introduced at the beginning of this section\n",
    "\n",
    "Whisper, unlike Wav2Vec2, operates with spectrograms. As it’s commonly done in sequence-to-sequence, we begin by padding and/or truncating a batch of audio samples to ensure all batches have the same input length. Padding is achieved by adding 0s at the end. The padded audios are then converted into a sequence of log-mel spectrograms by sliding a window, just like we saw in the first part of the chapter.\n",
    "\n",
    "It doesn’t just output the transcription but also different special characters that provide significant flexibility for Whisper to perform multiple tasks in multiple languages. This format is not just specific to the output texts but also to the data used during training. Some of the most important tokens are:\n",
    "\n",
    "The speech begins with a start of transcript token.\n",
    "\n",
    "If the language is not English, there is a language tag token (e.g., hi for Hindi).\n",
    "\n",
    "With the language tag, one can perform language identification, transcription, or translate to English.\n",
    "\n",
    "If there’s a no speech token, Whisper is used for voice activity detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\n",
    "    \"openai/whisper-small\", language=\"Spanish\", task=\"transcribe\"\n",
    ")\n",
    "\n",
    "input_str = \"Hola, ¿cómo estás?\"\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                         {input_str}\")\n",
    "print(f\"Formatted input w/ special:    {decoded_with_special}\")\n",
    "print(f\"Formatted input w/out special: {decoded_str}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
    "Input:                         Hola, ¿cómo estás?\n",
    "Formatted input w/ special:    <|startoftranscript|><|es|><|transcribe|><|notimestamps|>Hola, ¿cómo estás?<|endoftext|>\n",
    "Formatted input w/out special: Hola, ¿cómo estás?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    \"openai/whisper-base\"\n",
    ").to(device)\n",
    "\n",
    "inputs = whisper_processor(\n",
    "    array, sampling_rate=sampling_rate, return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    generated_ids = whisper_model.generate(**inputs.to(device))\n",
    "\n",
    "transcription = whisper_processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=False\n",
    ")[0]\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
    "<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Chapter 16. I might have told you of the beginning<|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genaibook.core import generate_long_audio\n",
    "from transformers import pipeline\n",
    "\n",
    "long_audio = generate_long_audio()\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\", model=\"openai/whisper-small\", device=device\n",
    ")\n",
    "pipe(\n",
    "    long_audio,\n",
    "    generate_kwargs={\"task\": \"transcribe\"},\n",
    "    chunk_length_s=5,\n",
    "    batch_size=8,\n",
    "    return_timestamps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'text': \" Chapter 16. I might have told you of the beginning of this liaison in a few lines, but I wanted you to see every step by which we came. I too agree to whatever Marguerite wished, margarite to be unable to live apart from me. It was the day after the evening when she came to see me that I sent her Manon the Scott. From that time, seeing that I could not change my mistress's life. I changed my own. I wished above all not to leave myself time to think over the position I had accepted, for, in spite of myself, it was a great distress to me. Thus my life, generally so calm, assumed all at once an appearance of noise and disorder. Never believe, however disinterested, the love of a kept woman may be, that it will cost one nothing. Nothing is so expensive as their caprices, flowers, boxes at the theater, suppers, days in the country, which one can never refuse to one's mistress. As I have told you, I had little money.\",\n",
    " 'chunks': [{'timestamp': (0.0, 25.13),\n",
    "   'text': ' Chapter 16. I might have told you of the beginning of this liaison in a few lines, but I wanted you to see every step by which we came. I too agree to whatever Marguerite wished, margarite to be unable to live apart from me. It was the day after the evening when she came to see me that I sent her Manon the Scott.'},\n",
    "  {'timestamp': (25.13, 29.07),\n",
    "   'text': \" From that time, seeing that I could not change my mistress's life.\"},\n",
    "  {'timestamp': (29.07, 66.63),\n",
    "   'text': \" I changed my own. I wished above all not to leave myself time to think over the position I had accepted, for, in spite of myself, it was a great distress to me. Thus my life, generally so calm, assumed all at once an appearance of noise and disorder. Never believe, however disinterested, the love of a kept woman may be, that it will cost one nothing. Nothing is so expensive as their caprices, flowers, boxes at the theater, suppers, days in the country, which one can never refuse to one's mistress.\"},\n",
    "  {'timestamp': (66.63, 68.95),\n",
    "   'text': ' As I have told you, I had little money.'}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genaibook.core import measure_latency_and_memory_use\n",
    "\n",
    "wav2vec2_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"facebook/wav2vec2-base-960h\",\n",
    "    device=device,\n",
    ")\n",
    "whisper_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\", model=\"openai/whisper-base\", device=device\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    measure_latency_and_memory_use(\n",
    "        wav2vec2_pipe, array, \"Wav2Vec2\", device, nb_loops=100\n",
    "    )\n",
    "    measure_latency_and_memory_use(\n",
    "        whisper_pipe, array, \"Whisper\", device=device, nb_loops=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
    "Wav2Vec2 execution time: 0.033196728515625 seconds\n",
    "Wav2Vec2 max memory footprint: 2.384644096 GB\n",
    "Whisper execution time: 0.126587021484375 seconds\n",
    "Whisper max memory footprint: 2.363074048 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")\n",
    "\n",
    "label = \"how can the llama jump\"\n",
    "pred = \"can the lama jump up\"\n",
    "wer = wer_metric.compute(references=[label], predictions=[pred])\n",
    "\n",
    "print(wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code example is optimized for explainability\n",
    "# The inference could be done in batches for speedup, for example.\n",
    "from datasets import Audio\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "\n",
    "def normalise(batch):\n",
    "    batch[\"norm_text\"] = normalizer(batch[\"sentence\"])\n",
    "    return batch\n",
    "\n",
    "\n",
    "def evaluate_model(pipe, lang=\"en\", samples_to_evaluate=200, whisper=False):\n",
    "    dataset = load_dataset(\n",
    "        \"mozilla-foundation/common_voice_13_0\",\n",
    "        lang,\n",
    "        split=\"test\",\n",
    "        streaming=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "    dataset = dataset.map(normalise)\n",
    "    dataset = dataset.take(samples_to_evaluate)\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        if whisper:\n",
    "            extra_kwargs = {\n",
    "                \"task\": \"transcribe\",\n",
    "                \"language\": f\"<|{lang}|>\",\n",
    "                \"max_new_tokens\": 100,\n",
    "            }\n",
    "            prediction = pipe(\n",
    "                sample[\"audio\"][\"array\"],\n",
    "                return_timestamps=True,\n",
    "                generate_kwargs=extra_kwargs,\n",
    "            )\n",
    "        else:\n",
    "            prediction = pipe(sample[\"audio\"][\"array\"])\n",
    "        predictions.append(normalizer(prediction[\"text\"]))\n",
    "        references.append(sample[\"norm_text\"])\n",
    "    return predictions, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_suite = [\n",
    "    [\"Wav2Vec2\", wav2vec2_pipe, \"en\"],\n",
    "    [\"Wav2Vec2\", wav2vec2_pipe, \"fr\"],\n",
    "    [\"Whisper\", whisper_pipe, \"en\"],\n",
    "    [\"Whisper\", whisper_pipe, \"fr\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_ACCESS_TOKEN=\"YOUR_HF_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=HF_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cer_metric = load(\"cer\")\n",
    "\n",
    "for config in eval_suite:\n",
    "    predictions, references = evaluate_model(\n",
    "        config[1], lang=config[2], whisper=config[0] == \"Whisper\"\n",
    "    )\n",
    "\n",
    "    wer = wer_metric.compute(references=references, predictions=predictions)\n",
    "    cer = cer_metric.compute(references=references, predictions=predictions)\n",
    "\n",
    "    print(f\"{config[0]} metrics for lang: {config[2]}. WER: {wer} , CER: {cer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading metadata...: 16372it [00:00, 44589.69it/s]\n",
    "Wav2Vec2 metrics for lang: en. WER: 0.44012772751463547 , CER: 0.22138524750538055\n",
    "Reading metadata...: 16114it [00:01, 14657.73it/s]\n",
    "Wav2Vec2 metrics for lang: fr. WER: 1.0099113197704748 , CER: 0.5745033112582781\n",
    "Reading metadata...: 16372it [00:00, 38628.98it/s]\n",
    "Whisper metrics for lang: en. WER: 0.2687599787120809 , CER: 0.14674232048522795\n",
    "Reading metadata...: 16114it [00:00, 36785.75it/s]\n",
    "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
    "Whisper metrics for lang: fr. WER: 0.5211267605633803 , CER: 0.2573583517292127"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
